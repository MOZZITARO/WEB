{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc9975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7892\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7892/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2218, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1729, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 894, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_6420\\3141663054.py\", line 76, in flim_chatbot\n",
      "    movie_info = flim(message)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_6420\\3141663054.py\", line 50, in flim\n",
      "    movie = results[0]\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2218, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1729, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 894, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_6420\\3141663054.py\", line 76, in flim_chatbot\n",
      "    movie_info = flim(message)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_6420\\3141663054.py\", line 50, in flim\n",
      "    movie = results[0]\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2218, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1729, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Project\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 894, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_6420\\3141663054.py\", line 76, in flim_chatbot\n",
      "    movie_info = flim(message)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_6420\\3141663054.py\", line 50, in flim\n",
      "    movie = results[0]\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "#ì˜í™” API\n",
    "#ì„ë² ë”© ë“± í•„ìš”ì—†ìŒ\n",
    "TDMB_key = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI5N2Y1ZWE1ZjEyZmEyZWJjYjdkMzFiZWI0YWQzMjhiMCIsIm5iZiI6MTc0OTYwNDg0NC4yMjIwMDAxLCJzdWIiOiI2ODQ4ZDllYzU0Nzc5ODExZjQzMDIyN2UiLCJzY29wZXMiOlsiYXBpX3JlYWQiXSwidmVyc2lvbiI6MX0._o4NNUpXCT3sQfi_jOdZW9wxgpY3tmZqhyNW9qK7Wpg\"\n",
    "# GEMINI_Key = \"AIzaSyCnRq56TkrunOXlBscyV_7IkTRYvzoUUd8\"\n",
    "\n",
    "#ëª¨ë¸\n",
    "model = Ollama(model=\"gemma2\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# API ìš”ì²­ (í•¨ìˆ˜)\n",
    "# ì •ë³´ ë°›ì•„ì˜¤ê¸°\n",
    "def flim(title):\n",
    "    \n",
    "   #  ì§€ê¸ˆì²˜ëŸ¼ api_key=...ë¡œ URLì— ì§ì ‘ í‚¤ë¥¼ ë¶™ì—¬ ì“°ë©´, headersëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "   #  headers = {\n",
    "   #  \"accept\": \"application/json\",\n",
    "   #  \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI5N2Y1ZWE1ZjEyZmEyZWJjYjdkMzFiZWI0YWQzMjhiMCIsIm5iZiI6MTc0OTYwNDg0NC4yMjIwMDAxLCJzdWIiOiI2ODQ4ZDllYzU0Nzc5ODExZjQzMDIyN2UiLCJzY29wZXMiOlsiYXBpX3JlYWQiXSwidmVyc2lvbiI6MX0._o4NNUpXCT3sQfi_jOdZW9wxgpY3tmZqhyNW9qK7Wpg\"\n",
    "   #  }\n",
    "    \n",
    "    # url = f\"https://api.themoviedb.org/3/search/movie?api_key={TDMB_key}&query={title}&language=ko-KR\"\n",
    "    url = f\"https://api.themoviedb.org/3/search/movie\"\n",
    "    \n",
    "    params = {\n",
    "            'api_key': TDMB_key,\n",
    "            'query': title,\n",
    "            'language': 'ko-KR'\n",
    "        }\n",
    "        \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # response = requests.get(url)\n",
    "    # .get('results', [])ëŠ” í‚¤ê°€ ì—†ì–´ë„ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•´ì„œ ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤.\n",
    "    # results = response.json().get('results', [])\n",
    "    data = response.json()\n",
    "    results = data.get('results', [])\n",
    "    \n",
    "#     if not results:\n",
    "#          return f\"'{title}'ì— ëŒ€í•œ ì˜í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "      \n",
    "      \n",
    "    movie = results[0]\n",
    "#     overview = movie.get('overview', 'ì¤„ê±°ë¦¬ ì—†ìŒ')\n",
    "#     release_date = movie.get('release_date', 'ê°œë´‰ì¼ ì—†ìŒ')\n",
    "#     vote_average = movie.get('vote_average', 'í‰ì  ì—†ìŒ')\n",
    "\n",
    "#     return f\"\"\"ğŸ¬ ì œëª©: {movie.get('title', title)}\n",
    "#                   ğŸ“… ê°œë´‰ì¼: {release_date}\n",
    "#                   â­ í‰ì : {vote_average}\n",
    "#                   ğŸ“– ì¤„ê±°ë¦¬: {overview}\n",
    "#                   \"\"\"\n",
    "    movie_info = f\"\"\"ğŸ¬ ì œëª©: {movie.get('title', 'ì œëª© ì—†ìŒ')}\n",
    "ğŸ“… ê°œë´‰ì¼: {movie.get('release_date', 'ê°œë´‰ì¼ ì—†ìŒ')}\n",
    "â­ í‰ì : {movie.get('vote_average', 'í‰ì  ì—†ìŒ')}/10\n",
    "ğŸ“– ì¤„ê±°ë¦¬: {movie.get('overview', 'ì¤„ê±°ë¦¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}\n",
    "\"\"\"\n",
    "    return movie_info\n",
    "   \n",
    "\n",
    "# ì²´ì¸\n",
    "# def chain (model, flim):\n",
    "\n",
    "# ì˜í™” ì •ë³´ì—†ìŒ\n",
    "# ì±—ë´‡ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸\n",
    "def flim_chatbot(message):\n",
    "      \n",
    "      # 1. TMDBë¡œë¶€í„° ì˜í™” ì •ë³´ ë°›ì•„ì˜¤ê¸°\n",
    "      movie_info = flim(message)\n",
    "   \n",
    "      # messages = [\n",
    "      #    {\n",
    "      #          'role': 'system', \n",
    "      #          'content': 'ë‹¹ì‹ ì€ ì¹œê·¼í•œ ì˜í™” ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤. ì˜í™” ì •ë³´ë¥¼ ì¬ë¯¸ìˆê²Œ ì†Œê°œí•´ì£¼ì„¸ìš”.'\n",
    "      #    },\n",
    "      #    {\n",
    "      #          'role': 'user',\n",
    "      #          'content': message\n",
    "      #    }\n",
    "      # ]\n",
    "      \n",
    "      \n",
    "      # 2. AI ëª¨ë¸ì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "      prompt = f\"\"\"ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ì˜í™” ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤. \n",
    "      ë‹¤ìŒ ì˜í™” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¬ë¯¸ìˆê³  ìœ ìµí•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "      ì˜í™” ì •ë³´:\n",
    "      {movie_info}\n",
    "\n",
    "      ì‚¬ìš©ì ì§ˆë¬¸: {message}\n",
    "\n",
    "      ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜í™”ì— ëŒ€í•´ ì¹œê·¼í•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "      \n",
    "           \n",
    "      response = model.invoke(prompt)\n",
    "      # response = model.chat(messages=messages)\n",
    "      \n",
    "      return response\n",
    "      # return response['message']['content']\n",
    "    \n",
    "# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ìš© í•¨ìˆ˜ (history ë§¤ê°œë³€ìˆ˜ í•„ìš”)\n",
    "# def movie_chatbot(message, history):\n",
    "#     \"\"\"ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ìš© ì˜í™” ì±—ë´‡ í•¨ìˆ˜\"\"\"\n",
    "#     return flim_chatbot(message)\n",
    "\n",
    "# ê·¸ë¼ë””ì˜¤ ìƒì„±\n",
    "# def create_interface():\n",
    "iface = gr.Interface(\n",
    "        # í•¨ìˆ˜\n",
    "        fn=flim_chatbot,\n",
    "        inputs=gr.Textbox(lines=10),\n",
    "        outputs=gr.Textbox(lines=20),\n",
    "        title=\"ğŸ¬ ì˜í™” ì •ë³´ ì±—ë´‡\",\n",
    "        description=\"ì˜í™” ì œëª©ì„ ì…ë ¥í•˜ë©´ ìƒì„¸í•œ ì •ë³´ì™€ í•¨ê»˜ AIê°€ ì˜í™”ì— ëŒ€í•´ ì„¤ëª…í•´ë“œë¦½ë‹ˆë‹¤!\",\n",
    ")\n",
    "\n",
    "# iface = create_interface()\n",
    "iface.launch(server_port=7892, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d62dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
