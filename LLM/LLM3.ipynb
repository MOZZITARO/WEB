{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78aaab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_ollama(question):\n",
    "    chatbot_role = \" 너는 전문 심리 상담가야, 질문 답은 3줄 이내로 짧게해줘 \"\n",
    "    response = ollama.chat(model='gemma2', messages=[\n",
    "        \n",
    "        {\"role\": \"system\", \"content\": chatbot_role}, #챗봇의 기본 역할\n",
    "        {\"role\": \"user\", \"content\":question}, #질문\n",
    "        \n",
    "        \n",
    "    ])\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171de2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\.venv\\lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://0e2391f20bbf6f64c1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0e2391f20bbf6f64c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage   # HumanMessage: 사용자가 보낸 메시지, AIMessage : LLM의 메시지\n",
    "import gradio as gr\n",
    "\n",
    "# ollama 모델 초기화(지정)\n",
    "model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False) # tem 낮을수록 동일 # 높을수록 창의적\n",
    "\n",
    "# 채팅기록 포함하여 응답 생성\n",
    "def chat(message, history):\n",
    "    \n",
    "    # 이전 대화 기록을 ChatOllama 형식으로 변환\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "    \n",
    "    # 현재 메시지 추가\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "    \n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = model.invoke(chat_history)\n",
    "    \n",
    "    return response.content   \n",
    "\n",
    "chatbot = gr.ChatInterface(\n",
    "    fn =chat,\n",
    "    examples=[\n",
    "        \"안녕하세요!\",\n",
    "        \"인공지능에 대해 설명해주세요.\",\n",
    "        \"파이썬의 장점은 무엇인가요?\"\n",
    "    ],\n",
    "    title=\"AI 챗봇\",\n",
    "    description=\"질문을 입력하면 AI가 답변합니다.\"  \n",
    ")\n",
    "\n",
    "#서버 실행\n",
    "chatbot.launch(server_port=7861, server_name=\"0.0.0.0\", share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb71b59",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/indata_kor.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# CSV 파일 로드\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset/indata_kor.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCP949\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 텍스트 나누기\u001b[39;00m\n\u001b[0;32m     14\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[1;32mc:\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Project\\.venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/indata_kor.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage   # HumanMessage: 사용자가 보낸 메시지, AIMessage : LLM의 메시지\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter  # 특정 문자(예: 줄바꿈, 공백)를 기준으로 텍스트를 분할하는 기능을 제공\n",
    "from langchain.chains import ConversationalRetrievalChain  # 질문과 관련된 정보 검색, 이전 대화정보도 같이 LLM에 제공, 답변생성\n",
    "import gradio as gr\n",
    "\n",
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"./dataset/indata_kor.csv\", encoding='CP949')\n",
    "\n",
    "# 텍스트 나누기\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df.to_string()))\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "# 경량화 버전\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 벡터화 저장소\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "# chatollama 모델 초기화\n",
    "llm = ChatOllama(model=\"gemma2\", temperature=0.1)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 채팅 설정(함수)\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ConversationalRetrievalChain형식으로 변환\n",
    "    chat_history = [(human, ai) for human, ai in history]\n",
    "    \n",
    "    # 모델을 사용하여 응답 생성 \n",
    "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "    \n",
    "    # 소스 문서 정보 추출\n",
    "    source = set([doc.metadata.get('source', 'Unknown') for doc in response['source_documents']])\n",
    "    source_info = f\"\\n\\n참고 출처: {', '.join(source)}\" if source else \"\"\n",
    "\n",
    "    return response['answer'] + source_info\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?\",\n",
    "        \"스마트금융과에 대해 설명해주세요\",\n",
    "        \"한국폴리텍대한 추천할만한 학과 하나를 소개해주세요.\"\n",
    "    ],\n",
    "    title=\"대학 정보 AI 챗봇\",\n",
    "    description=\"스마트금융과에 대한 질문을 입력하면 AI가 CSV데이터를 참고하여 한글로 답변합니다.\"\n",
    ")\n",
    "\n",
    "# 서버 실행\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7982893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "\n",
    "\n",
    "# 인터넷 정보 끌어오기\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "    \n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "    \n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Function to format documents\n",
    "\n",
    "def format_docs(docs):\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG 체인 \n",
    "def rag_chain(url, question):\n",
    "    \n",
    "    retriever = load_and_retrieve_docs(url)\n",
    "    \n",
    "    retrived_docs = retriever.invoke(question)\n",
    "    \n",
    "    formatted_context = format_docs(retrived_docs)\n",
    "    \n",
    "    formatted_prompt = f\"Question: {question} \\n\\n Context: {formatted_context}\"\n",
    "    \n",
    "    response = ollama.chat(model='gemma2', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "st = gr.Interface(\n",
    "        fn=rag_chain,\n",
    "        inputs=[\"text\", \"text\"],\n",
    "        outputs=\"text\",\n",
    "        title=\"RAG Chain Question Answering\",\n",
    "        description=\"URL과 질문을 입력하면 RAG 체인에서 답변을 받으실 수 있습니다.\"\n",
    "    )\n",
    "\n",
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "st.launch(server_port=7862, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8995974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7864\n",
      "* Running on public URL: https://b534a1145f968a9695.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b534a1145f968a9695.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  # 환경변수 로드가 필요한 경우\n",
    "import whisper\n",
    "import gradio as gr\n",
    "\n",
    "# ffmpeg 경로 명시적 설정\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\aiproject\\ffmpeg\\bin\"\n",
    "os.environ[\"FFMPEG_BINARY\"] = r\"C:\\aiproject\\ffmpeg\\bin\\ffmpeg.exe\"\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    # 모델 로드\n",
    "    model = whisper.load.model(\"base\")\n",
    "    \n",
    "    # 파일 전사 (받아적기)\n",
    "    result = model.transcribe(audio_path)\n",
    "    \n",
    "    # 전사 텍스트 반환\n",
    "    return result[\"text\"]\n",
    "\n",
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"오디오 파일을 업로드해주세요.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"MP3 파일 업로드\"),\n",
    "    outputs=\"text\",\n",
    "    title = \"MP3 to Text Converter\",\n",
    "    description=\"MP3 파일을 업로드하면 텍스트로 변환합니다.\"\n",
    ")\n",
    "\n",
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7864, server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf23a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset3.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def text_to_speech(text, lang='ko'):\n",
    "    # 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:\n",
    "        temp_filename = fp.name\n",
    "    \n",
    "    # TTS 변환\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename\n",
    "\n",
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"텍스트를 입력해주세요.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"변환이 완료되었습니다. 아래에서 재생 또는 다운로드할 수 있습니다.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"오류가 발생했습니다: {str(e)}\"\n",
    "    \n",
    " # Gradion 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn = process_tts,\n",
    "    inputs = [gr.Textbox(lines=5, label=\"텍스트 입력\"),\n",
    "              gr.Dropdown(choices=['ko', 'en', 'ja', 'zh-cn'], label=\"언어 선택\", value='ko')\n",
    "    ],\n",
    "    outputs= [gr.Audio(label=\"생성된 오디오\"),\n",
    "              gr.Textbox(label=\"상태 메시지\")\n",
    "    ],\n",
    "    title = \"Text to Speech Converter\",\n",
    "    description=\"텍스트를 입력하연 MP3 파일로 변환합니다.\"\n",
    ")\n",
    "\n",
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7868, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c26ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f335a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\u001b[1m14536120/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "* Running on local URL:  http://0.0.0.0:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO #  메모리에서 바이트 데이터를 저장하고 읽는 기능\n",
    "\n",
    "model = tf.keras.applications.MobileNetV2(weights=\"imagenet\")\n",
    "\n",
    "def predict_image(image_url):\n",
    "    try:\n",
    "        # 이미지 가져오기\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content)).resize((224, 224))  # BytesIO 사용하여 이미지 열기\n",
    "        \n",
    "        # 이미지를 배열로\n",
    "        image_array = tf.keras.preprocessing.image.img_to_array(image) # 이미지를 숫자 배열로 전환\n",
    "        image_array = tf.expand_dims(image.array, axis=0) # 모델이 한 번에 여러 이미지를 처리하게 \"배치\" 차원 추가\n",
    "        image_array = tf.keras.applications.mobilenet_v2.preprocess_input(image_array)  # 이미지 픽셀 값을 모델이 학습할 때 사용했던 범위로 조정 전처리\n",
    "        \n",
    "        # 예측 수행\n",
    "        predictions = model.predict(image_array)\n",
    "        decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=3)[0]\n",
    "        \n",
    "        # Gradio Label 컴포넌트에 맞게 결과 형식 변경\n",
    "        # {label1: confidence1, label2: confidence2, ...} 형식으로 반환\n",
    "        result = {label: float(prob) for (_, label, prob) in decoded_predictions}\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": 1.0}  # 에러 발생 시 기본값 반환\n",
    "    \n",
    "iface = gr.Interface(\n",
    "fn=predict_image,\n",
    "inputs=gr.Textbox(label=\"이미지 URL 입력\"),\n",
    "                 # 상위 3개\n",
    "outputs=gr.Label(num_top_classes=3, label=\"예측 결과\"),\n",
    "title=\"음식 이미지 분류\",\n",
    "description=\"이미지 URL을 입력하면 상위 3개의 예측 결과를 확률과 함께 표시합니다.\"\n",
    ")\n",
    "\n",
    "# 인터페이스 실행\n",
    "iface.launch(server_port=7869, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2409825",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d84cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# TensorFlow MobileNetV2 모델 로드\n",
    "model = tf.keras.applications.MobileNetV2(weights=\"imagenet\")  # 사전 훈련된 가중치를 사용\n",
    "\n",
    "# 다른 점\n",
    "OLLAMA_SERVER = \"http://localhost:11434\"  # 로컬 서버 주소\n",
    "MODEL_NAME = \"gemma2\"  # 사용하려는 Ollama 모델 이름\n",
    "\n",
    "# Ollama를 사용해 음식 설명 생성\n",
    "# 다른 점 2\n",
    "def get_food_description_with_langchain(food_name):\n",
    "    \"\"\"\n",
    "    LangChain ChatOllama를 사용하여 음식 설명 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chat = ChatOllama(base_url=OLLAMA_SERVER, model=MODEL_NAME)\n",
    "        prompt = f\"{food_name}에 대해 특징, 효능, 요리 레시피 설명해줘.설명은 한국어로 해줘.\"\n",
    "        response = chat.predict(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Failed to retrieve description: {e}\"\n",
    "    \n",
    "    # 이미지 예측 함수\n",
    "def predict_image_with_description(image_url):\n",
    "    \"\"\"\n",
    "    이미지 URL을 받아 음식 예측과 Ollama 설명을 반환\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL에서 이미지 가져오기\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content)).resize((224, 224))  # BytesIO 사용하여 이미지 열기\n",
    "\n",
    "        # 이미지를 배열로 변환\n",
    "        image_array = tf.keras.preprocessing.image.img_to_array(image)  # 이미지를 숫자 배열로 전환\n",
    "        image_array = tf.expand_dims(image_array, axis=0)  # 모델이 한 번에 여러 이미지를 처리할 수 있게 \"배치\"라는 차원을 추가\n",
    "        image_array = tf.keras.applications.mobilenet_v2.preprocess_input(image_array)  # 이미지 픽셀 값을 모델이 학습할 때 사용했던 범위로 조정 전처리\n",
    "\n",
    "        # 예측 수행\n",
    "        predictions = model.predict(image_array)\n",
    "        decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=3)[0]  # 상위 3개 예측 결과 반환\n",
    "\n",
    "        # 예측 결과 형식화\n",
    "        result = {label: float(prob) for (_, label, prob) in decoded_predictions} # 예측 결과를 Gradio의 Label 컴포넌트가 요구하는 형식으로 변환\n",
    "\n",
    "        # 가장 높은 확률의 예측값으로 Ollama 설명 생성\n",
    "        top_food = decoded_predictions[0][1]  # 가장 확률이 높은 음식 이름\n",
    "        description = get_food_description_with_langchain(top_food)\n",
    "\n",
    "        return result, description  # 예측 결과와 Ollama 설명 반환\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": 1.0}, f\"Error: {e}\"  # 에러 발생 시 기본값 반환\n",
    "    \n",
    "    # Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=predict_image_with_description,\n",
    "    inputs=gr.Textbox(label=\"이미지 URL 입력\"),\n",
    "    outputs=[\n",
    "        gr.Label(num_top_classes=3, label=\"예측 결과\"),  # 상위 3개 예측 결과\n",
    "        gr.Textbox(label=\"음식 설명\", interactive=False)  # Ollama로 생성한 설명 출력\n",
    "    ],\n",
    "    title=\"음식 이미지 분류 및 설명 생성기\",\n",
    "    description=\"이미지 URL을 입력하면 음식 분류 결과와 설명을 제공합니다.\"\n",
    ")\n",
    "\n",
    "# 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager # 다양한 이벤트에 대한 콜백을 관리하는 클래스\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # 텍스트 생성되는 대로 바로 출력\n",
    "import re  # 텍스트 패턴을 정의 (텍스트를 검색, 치환, 분리)\n",
    "from typing import Dict, Any  # 타입을 명시적으로 지정하여 코드의 가독성과 안정성을 높임\n",
    "import json\n",
    "\n",
    "# DB 연결 설정\n",
    "DB_URL = \"mysql+pymysql://root:kirito1013@localhost:3306/test\"\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "class EnhancedQueryGenerator:\n",
    "    def __init__(self):\n",
    "        self.query_template= \"\"\"\n",
    "        당신은 한국어를 잘하고 MySQL 데이터베이스의 쿼리를 생성하는 전문가입니다.\n",
    "        데이터베이스 스키마 정보:\n",
    "        {schema_info}\n",
    "\n",
    "        이전 피드백 정보:\n",
    "        {feedback_info}\n",
    "\n",
    "        위 정보를 바탕으로 다음 질문에 대한 MySQL 쿼리를 생성해주세요.\n",
    "        질문: {question}\n",
    "\n",
    "        규칙:\n",
    "        1. 순수한 SQL 쿼리만 작성하세요\n",
    "        2. 컬럼의 실제 값을 기준으로 쿼리를 작성하세요\n",
    "        3. 설명이나 주석을 포함하지 마세요\n",
    "        4. 쿼리는 SELECT 문으로 시작하고 세미콜론(;)으로 끝나야 합니다\n",
    "        5. WHERE 절에서는 정확한 값 매칭을 위해 = 연산자를 사용하세요\n",
    "        6. 유사 검색이 필요한 경우 LIKE '%키워드%' 를 사용하세요\n",
    "        7. 관련된 모든 결과를 찾기 위해 적절히 OR 조건을 활용하세요\n",
    "        \"\"\"\n",
    "        \n",
    "        self.answer_template = \"\"\"\n",
    "        다음 정보를 바탕으로 사용자의 질문에 대한 답변을 생성해주세요:\n",
    "\n",
    "        원래 질문: {question}\n",
    "        실행된 쿼리: {query}\n",
    "        쿼리 결과: {result}\n",
    "\n",
    "        규칙:\n",
    "        1. 결과를 자연스러운 한국어로 설명해주세요\n",
    "        2. 숫자 데이터가 있다면 적절한 단위와 함께 표현해주세요\n",
    "        3. 결과가 없다면 그 이유를 설명해주세요\n",
    "        4. 전문적인 용어는 쉽게 풀어서 설명해주세요\n",
    "        \"\"\"\n",
    "        \n",
    "        #모델 설정\n",
    "        callback_manager = CallbackManager({StreamingStdOutCallbackHandler()})\n",
    "        self.llm = Ollama(\n",
    "            model = \"gemma2\",\n",
    "            temperature = 0.\n",
    "            callback_manager=callvack_manager\n",
    "        )\n",
    "        \n",
    "        # 프롬프트 템플릿\n",
    "        self.query_prompt = ChatPromptTemplate.from_template(self.query_template)\n",
    "        self.query_prompt = ChatPromptTemplate.from_template(self.answer_template)\n",
    "        \n",
    "        # chain\n",
    "        self.query_chain = LLMChain(llm=self.llm, prompt=self.query_prompt)\n",
    "        self.query_chain = LLMChain(llm=self.llm, prompt=self.answer_prompt)\n",
    "        \n",
    "        def generate_query(self, question: str, schema_info: str, feedback_info: str = \"\") -> str:\n",
    "            \"\"\"질문에 대한 SQL 쿼리를 생성합니다.\"\"\"\n",
    "            response = self.query_chain.run(\n",
    "                question=question,\n",
    "                schema_info=schema_info\n",
    "                feedback_info=feedback_info\n",
    "            )\n",
    "            return self.extract_sql_query(response)\n",
    "        \n",
    "        def generate_answer(self, question: str, query: str, result: Any) -> str:\n",
    "        \"\"\"쿼리 결과를 바탕으로 자연어 답변을 생성합니다.\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Ollama 설정 (Gemma2 모델 사용)\n",
    "os.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\" # Ollama 서버 주소\n",
    "ollama_model = Ollama(model=\"gemma2\")\n",
    "\n",
    "# 템플릿\n",
    "TEMPLATES = {\n",
    "       \"취업\": \"다음 키워드와 예시를 바탕으로, 취업 지원을 위한 자기소개서를 작성하세요.\",\n",
    "    \"대학원\": \"제공된 키워드를 사용하여, 대학원 지원을 위한 자기소개서를 초안 작성하세요.\",\n",
    "    \"봉사활동\": \"주어진 키워드를 활용하여, 봉사활동 경험과 동기를 강조하는 자기소개서를 작성하세요.\"  \n",
    "}\n",
    "\n",
    "#언어 지원\n",
    "LANGUAGES = {\n",
    "    \n",
    "     \"한국어\": \"Please write the response in Korean.\",\n",
    "    \"영어\": \"Please write the response in English.\",\n",
    "    \"일본어\": \"Please write the response in Japanese.\"\n",
    "    \n",
    "}\n",
    "\n",
    "#자동 키워드 추천\n",
    "def recommend_keywords(purpose):\n",
    "    if purpose == \"취업\":\n",
    "        return \"책임감, 팀워크, 문제 해결 능력\"\n",
    "    elif purpose == \"대학원\":\n",
    "        return \"연구 열정, 창의력, 학업 성취도\"\n",
    "    elif purpose == \"봉사활동\":\n",
    "        return \"사회적 책임감, 희생정신, 리더십\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# 자기소개서 작성 함수\n",
    "def generate_statement(purpose, language, keywords, example_sentence=None):\n",
    "    if purpose not in TEMPLATES:\n",
    "        return \"지원목적을 올바르게 설정해주세요\"\n",
    "    if language not in LANGUAGES:\n",
    "        return \"언어를 올바르게 선택해주세요.\"\n",
    "    \n",
    "    #여기까지가 템플릿임\n",
    "    template = TEMPLATES[purpose] + \"\\n\\nKeywords: {Keywords}\\n\" + LANGUAGES[language]\n",
    "    if example_sentence:\n",
    "        template += f\"\\n\\nExample sentence : {example_sentence}\"\n",
    "        \n",
    "    prompt = PromptTemplate(input_variables=[\"keywords\"], template=template)\n",
    "    chain = LLMChain(llm=ollama_model, prompt=prompt)\n",
    "    response = chain.run({\"keywords\": keywords})\n",
    "    return response\n",
    "\n",
    "#PDF로 저장\n",
    "def pdf_save(statement, filename=\"personal_statement.pdf\"):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.add_font('MalgunGothic', '', r'C:\\Windows\\Fonts\\malgun.ttf', uni=True)  # '맑은 고딕' 폰트 경로 설정\n",
    "    pdf.set_font('MalgunGothic', size=12)  # 폰트 설정\n",
    "    pdf.multi_cell(0, 10, statement) # PDF 문서에 텍스트를 추가, 셀의너비(0), 셀의 높이(10)\n",
    "    pdf.output(filename) # PDF 문서를 파일로 저장\n",
    "    return f\"✔️ PDF 저장 완료: {filename}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40417ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import kss  # kss는 \"Korean Sentence Splitter\"의 약자로, 한국어 문장 분리 도구\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # 파이썬의 warnings 모듈을 사용하여 발생하는 경고 메시지를 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API 키 설정\n",
    "os.environ[\"GEMINI_API_KEY\"] = \" K  E  Y \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAnalyzer:\n",
    "    def __init__(self):\n",
    "        print(\"모델 초기화 중....\")\n",
    "        \n",
    "        #감성 분석 모델 설정\n",
    "        print(\"감성 분석 모델 로딩...\")\n",
    "        self.sentiment_analyzer = pipeline(\n",
    "            \n",
    "            \"sentiment-analysis\",\n",
    "            model = \"snunlp/KR-FinBert-SC\",\n",
    "            tokenizer=\"snunlp/KR-FinBert-SC\"\n",
    "            \n",
    "        )\n",
    "        \n",
    "        #가짜 뉴스 분류 모델\n",
    "        print(\"가짜 뉴스 분류 모델 로딩...\")\n",
    "        self.init_fake_news_classifier()\n",
    "        \n",
    "        print(\"모델 초기화 완료!\")\n",
    "        \n",
    "         #가짜 뉴스 분류 모델\n",
    "    def init_fake_news_classifier(self):\n",
    "        try:\n",
    "            # 'beomi/kcbert-base' 모델 사용 - 한국어에 더 특화된 모델\n",
    "            model_name = \"beomi/kcbert-base\"\n",
    "            self_fake_news_classifier = pipeline(\n",
    "                \"text-classification\",\n",
    "                model = model_name,\n",
    "                tokenizer=model_name\n",
    "            )\n",
    "            \n",
    "    def classify_long_text(text):\n",
    "        try: \n",
    "            #한국어 문장 분리\n",
    "            sentences = kss.split_sentences(text) # 텍스트를 문장단위로 분리\n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            #청크 (문장 덩어리 생성)\n",
    "            for sentence in sentences:\n",
    "                if len(current_chunk) + len(sentence) < 1000:\n",
    "                    current_chunk += sentence + \" \"\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sentence + \"\"\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                \n",
    "            if not chunks : \n",
    "                chunks = [text]\n",
    "                \n",
    "        # 청크 분류 수행\n",
    "        result = []\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                result = self.fake_news_classifier(chunk[:512])\n",
    "                \n",
    "                # 신뢰도 점수 계산\n",
    "                score = result[0]['score']\n",
    "                \n",
    "                # 가짜뉴스 특성 체크\n",
    "                fake_news_indicators = self.check_fake_news_indicators(chunk)\n",
    "                \n",
    "                # 최종 점수 조정\n",
    "                adjusted_score = self.adjust_score(score, fake_news_indicators)\n",
    "                result.append(adjusted_score)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"청크 처리 중 오류: {str(e)}\")\n",
    "                continue    \n",
    "            \n",
    "        # 결과 평균 계산 및 레이블 결정\n",
    "        if results: \n",
    "            avg_score = np.mean(results)\n",
    "            final_label = 'FAKE' if avg_score > 0.6 else 'REAL'\n",
    "            return [{}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
